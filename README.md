<h1 align="center">ğŸ–¼ï¸ Image Captioning using Vision Transformer + GPT-2</h1>

<p align="center">
An exciting <b>AI project</b> that generates human-like captions for images using the power of <b>Vision Transformer (ViT)</b> and <b>GPT-2</b>.  
This project beautifully blends <b>Computer Vision</b> ğŸ§  and <b>Natural Language Processing</b> ğŸ—£ï¸ to make machines describe what they see! ğŸ¤–âœ¨
</p>

---

<p align="center">
  <img src="https://img.shields.io/badge/Language-Python-blue?logo=python&logoColor=white">
  <img src="https://img.shields.io/badge/Level-Basic%20Python%20Completed-brightgreen">
  <img src="https://img.shields.io/badge/AI-Image%20Captioning-orange">
</p>

---

## ğŸ§  Overview

This project demonstrates **Image Captioning** â€” an AI technique that automatically generates descriptive sentences for images.  
It uses a **Vision Transformer (ViT)** as the image encoder and **GPT-2** as the text decoder, built using the pretrained model  
`nlpconnect/vit-gpt2-image-captioning` from Hugging Face.

---

## âš™ï¸ Features

- ğŸ§© Generates meaningful, human-like captions for any image  
- âš¡ Powered by **ViT-GPT2**, a state-of-the-art pretrained model  
- ğŸ’» Supports **GPU (CUDA)** for faster inference  
- ğŸ“· Works with all major image formats  
- ğŸ” Reusable function `predict_step()` for easy integration  

---

## ğŸ¯ Learning Objectives

By completing this project, you will:
- Learn how **AI connects vision and language**  
- Understand the use of **Transformers** in real-world applications  
- Practice **loading and using pretrained models** from Hugging Face  
- Strengthen your **Python and deep learning fundamentals**

---

## ğŸ§° Requirements

Install the following libraries before running the code:

```bash
pip install transformers torch pillow
